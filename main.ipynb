{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "from mediapipe import solutions\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = HandLandmarkerOptions(\n",
    "    base_options = BaseOptions(model_asset_path=\"hand_landmarker.task\"),\n",
    "    running_mode = VisionRunningMode.IMAGE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = vision.HandLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a detector object's hand_landmarks list (detecting the hand as a whole in the image)\n",
    "#takes in the single_images numpyview off of mp create image\n",
    "\n",
    "def draw_landmarks_on_hand(image, hand_landmarks_list):\n",
    "    modified_image = np.copy(image)\n",
    "    \n",
    "    if len(hand_landmarks_list) > 0:\n",
    "        points = []\n",
    "        for hand_point in hand_landmarks_list[0]:\n",
    "            hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "\n",
    "            hand_landmarks_proto.landmark.extend([\n",
    "                landmark_pb2.NormalizedLandmark(x=hand_point.x, y=hand_point.y, z=hand_point.z) for hand_point in hand_landmarks_list[0]\n",
    "            ])\n",
    "            \n",
    "            points.append([hand_point.x, hand_point.y, hand_point.z])\n",
    "\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                modified_image,\n",
    "                hand_landmarks_proto,\n",
    "                solutions.hands.HAND_CONNECTIONS,\n",
    "                solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "                solutions.drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "        return True, modified_image, np.array(points)\n",
    "    else:\n",
    "        #no hand detected\n",
    "        return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_image = mp.Image.create_from_file(single_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tensorflow.keras.models.load_model(\"sign_language_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dictionary = {0:\"Hello!\", 1: \"I Love You!\", 2: \"Yes!\"} #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "        detection_results = detector.detect(mp_image)\n",
    "        success, modified_frame, points = draw_landmarks_on_hand(frame, detection_results.hand_landmarks) #pass in only one hand detected\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        if success:\n",
    "            points = points.reshape(1, 21, 3)\n",
    "            prediction = model.predict(points)\n",
    "            cv2.putText(modified_frame, label_dictionary[np.argmax(prediction, axis=1)[0]], (250, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"sign detection\", modified_frame)\n",
    "        else:\n",
    "            cv2.putText(frame, \"No hand detected yet!\", (25, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "            cv2.imshow(\"sign detection\", frame)        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
